{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1ba3738-5f72-4502-8a02-6b79500e2133",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%fs mkdirs /mnt/Avocado_Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98be3392-8baa-4eaf-99b0-b3d11d492fa4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#Config Padrão\n",
    "\n",
    "configs = {\"fs.azure.account.auth.type\": \"OAuth\",\n",
    " 
    "\n",
    "# Optionally, you can add <directory-name> to the source URI of your mount point.\n",
    "dbutils.fs.mount(\n",
    "  source = \"abfss://avocado@avocadoaccount.dfs.core.windows.net/\",\n",
    "  mount_point = \"/mnt/Avocado_Project\",\n",
    "  extra_configs = configs)\n",
    "\n",
    "#Substitua\n",
    "\n",
    "#<application-id> com a ID de aplicativo (cliente) para o aplicativo do Azure Active Directory.\n",
    "#<scope-name> pelo nome do escopo de segredo do Databricks.\n",
    "#<service-credential-key-name> com o nome da chave que contém o segredo do cliente.\n",
    "#<directory-id> com a ID do Diretório (locatário) ID para o aplicativo do Azure Active Directory.\n",
    "#<container-name> pelo nome de um contêiner na conta de armazenamento do ADLS Gen2.\n",
    "#<storage-account-name> pelo nome da conta de armazenamento do ADLS Gen2.\n",
    "#<mount-name> com o nome do ponto de montagem pretendido no DBFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf74da51-0baf-4950-bfcc-893863d4d011",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%fs ls /mnt/Avocado_Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2157361b-7ec2-45ec-945c-c5f389d32856",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"dbfs:/mnt/Avocado_Project/Bronze/avocado_dataset.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b5d0a31-17bb-4347-b4aa-0d1be3718c43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Changing the name of the column\n",
    "df = df\\\n",
    "    .withColumnRenamed(\"date\", \"Date\")\\\n",
    "    .withColumnRenamed(\"average_price\", \"AveragePrice\")\\\n",
    "    .withColumnRenamed(\"total_volume\", \"Volume\")\\\n",
    "    .withColumnRenamed(\"4046\", \"SmallHass\")\\\n",
    "    .withColumnRenamed(\"4225\", \"LargeHass\")\\\n",
    "    .withColumnRenamed(\"4770\", \"XLargeHass\")\\\n",
    "    .withColumnRenamed(\"total_bags\", \"TotalBags\")\\\n",
    "    .withColumnRenamed(\"small_bags\", \"SmallBags\")\\\n",
    "    .withColumnRenamed(\"large_bags\", \"LargeBags\")\\\n",
    "    .withColumnRenamed(\"x_large_bags\", \"XLargeBags\")\\\n",
    "    .withColumnRenamed(\"type\", \"Type\")\\\n",
    "    .withColumnRenamed(\"year\", \"Year\")\\\n",
    "    .withColumnRenamed(\"geography\", \"Region\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "341aabce-85e4-4483-9b93-aee3c2d0e596",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import (to_date, year, month, dayofweek, col, lit, when)\n",
    "\n",
    "df = df.withColumn(\"date\", to_date(col(\"Date\"), \"MM/dd/yyyy\"))\n",
    "df = df.withColumn(\"Year\", year(col(\"date\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3625e6b5-3b44-40b3-80de-2b13d92df58e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "null_counts = df.select(\n",
    "    [sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f580d786-1901-4f97-91a7-aa028945ef78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "initial_count = df.count()\n",
    "df_no_duplicates = df.dropDuplicates()\n",
    "duplicate_count = initial_count - df_no_duplicates.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5090b9c5-92c7-4064-9447-961dc1f043bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "\n",
    "# Indexador para 'type'\n",
    "indexer_type = StringIndexer(inputCol=\"type\", outputCol=\"type_index\")\n",
    "df = indexer_type.fit(df).transform(df)\n",
    "\n",
    "# Indexador para 'region'\n",
    "indexer_region = StringIndexer(inputCol=\"region\", outputCol=\"region_index\")\n",
    "df = indexer_region.fit(df).transform(df)\n",
    "\n",
    "# OneHotEncoder\n",
    "encoder = OneHotEncoder(\n",
    "    inputCols=[\"type_index\", \"region_index\"],\n",
    "    outputCols=[\"type_vec\", \"region_vec\"]\n",
    ")\n",
    "\n",
    "df_encoded = encoder.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba0f1f88-4884-4864-931c-ec1487f00305",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Após aplicar StringIndexer e OneHotEncoder\n",
    "df_encoded = df_encoded.drop(\"type\", \"region\", \"type_index\", \"region_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9de5242f-b376-41ec-98bc-1901ad2376de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Salvar versão para análises (com colunas originais categóricas)\n",
    "df.write.mode(\"overwrite\").parquet(\"dbfs:/mnt/Avocado_Project/Silver/avocado_silver.parquet\")\n",
    "\n",
    "# Salvar versão pronta para machine learning (com colunas vetorizadas)\n",
    "df_encoded.write.mode(\"overwrite\").parquet(\"dbfs:/mnt/Avocado_Project/Silver/avocado_silver_ml_ready.parquet\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8274977738232345,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Camada Silver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
