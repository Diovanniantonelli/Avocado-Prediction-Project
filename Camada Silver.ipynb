# %%
# Create directory in DBFS for the project
%fs mkdirs /mnt/Avocado_Project

# %%
# ðŸš« Secure Mounting Configuration Placeholder
# WARNING: Secrets have been removed from this notebook for safety.

# To securely mount your Azure Data Lake, use Databricks Secret Scopes.
# Example configuration (DO NOT HARD-CODE SECRETS):

# configs = {
#     "fs.azure.account.auth.type": "OAuth",
#     "fs.azure.account.oauth.provider.type": "org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider",
#     "fs.azure.account.oauth2.client.id": dbutils.secrets.get(scope="myscope", key="client-id"),
#     "fs.azure.account.oauth2.client.secret": dbutils.secrets.get(scope="myscope", key="client-secret"),
#     "fs.azure.account.oauth2.client.endpoint": "https://login.microsoftonline.com/<directory-id>/oauth2/token"
# }

# dbutils.fs.mount(
#     source = "abfss://<container-name>@<storage-account>.dfs.core.windows.net/",
#     mount_point = "/mnt/Avocado_Project",
#     extra_configs = configs
# )

# %%
# Check directory contents
%fs ls /mnt/Avocado_Project

# %%
# Load the CSV file from the Bronze layer
df = spark.read.csv("dbfs:/mnt/Avocado_Project/Bronze/avocado_dataset.csv", header=True, inferSchema=True)

# %%
# Rename columns for better readability
df = df\
    .withColumnRenamed("date", "Date")\
    .withColumnRenamed("average_price", "AveragePrice")\
    .withColumnRenamed("total_volume", "Volume")\
    .withColumnRenamed("4046", "SmallHass")\
    .withColumnRenamed("4225", "LargeHass")\
    .withColumnRenamed("4770", "XLargeHass")\
    .withColumnRenamed("total_bags", "TotalBags")\
    .withColumnRenamed("small_bags", "SmallBags")\
    .withColumnRenamed("large_bags", "LargeBags")\
    .withColumnRenamed("x_large_bags", "XLargeBags")\
    .withColumnRenamed("type", "Type")\
    .withColumnRenamed("year", "Year")\
    .withColumnRenamed("geography", "Region")

# %%
# Convert Date to Spark DateType and extract Year
from pyspark.sql.functions import to_date, year, col
df = df.withColumn("date", to_date(col("Date"), "MM/dd/yyyy"))
df = df.withColumn("Year", year(col("date")))

# %%
# Check for null values
from pyspark.sql.functions import sum as _sum
null_counts = df.select([_sum(col(c).isNull().cast("int")).alias(c) for c in df.columns])

# %%
# Drop duplicates and count how many were removed
initial_count = df.count()
df_no_duplicates = df.dropDuplicates()
duplicate_count = initial_count - df_no_duplicates.count()

# %%
# Encode categorical variables using StringIndexer and OneHotEncoder
from pyspark.ml.feature import StringIndexer, OneHotEncoder

# Type index
indexer_type = StringIndexer(inputCol="Type", outputCol="type_index")
df = indexer_type.fit(df).transform(df)

# Region index
indexer_region = StringIndexer(inputCol="Region", outputCol="region_index")
df = indexer_region.fit(df).transform(df)

# OneHotEncoder
encoder = OneHotEncoder(
    inputCols=["type_index", "region_index"],
    outputCols=["type_vec", "region_vec"]
)
df_encoded = encoder.fit(df).transform(df)

# %%
# Drop original categorical columns and their indexed versions
df_encoded = df_encoded.drop("Type", "Region", "type_index", "region_index")

# %%
# Save cleaned dataset for analysis (Silver Layer)
df.write.mode("overwrite").parquet("dbfs:/mnt/Avocado_Project/Silver/avocado_silver.parquet")

# Save machine learning-ready dataset (Silver Layer)
df_encoded.write.mode("overwrite").parquet("dbfs:/mnt/Avocado_Project/Silver/avocado_silver_ml_ready.parquet")
